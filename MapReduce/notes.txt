Some notes with regard to the execution on the Hortonworks sandbox:

1. You need to initialize the HDFS file system. It is done in the following way:
hdfs dfs -mkdir /user/<username>

As the username is root:
hdfs dfs -mkdir /user/root

2. The jar file can be found in /usr/hdp/current/hadoop-mapreduce-client

3. The cluster does have a name and it's not localhost. The hostname is sandbox.hortonworks.com (as extracted from
/etc/hosts) and also 127.0.0.2 can be used.

4. As the host machine I'm using is Windows, dos2unix has to be installed in the sandbox:
yum install dos2unix

5. python files must be made executable:
chmod a+x ./mapper.py
chmod a+x ./reducer.py

6. Before executing, they must be converted to Linux format:
dos2unix mapper.py
dos2unix reducer.py

7. Mind that the -files option requires that there is no space after the comma:
-files mapper.py,reducer.py

[8]. Install package us
cd /usr/bin
sudo yum install python-pip
pip install us

9. Clean former outputs:
hdfs dfs -rmr /user/root/output

10. Run (in the same line; for the hashtag option use '--type hashtags' instead).
10. a) Test with dump.txt and 'states' as option.
hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar
-files mapper_ng.py,reducer_ng.py,AFINN-111.txt,us_states.txt
-mapper mapper_ng.py -reducer 'reducer_ng.py --type states'
-input hdfs://sandbox.hortonworks.com/user/root/input/streaming/dump.txt
-output hdfs://sandbox.hortonworks.com/user/root/output/streaming_01

10. b) Real execution with dump2.txt and 'states' as option.
hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar
-files mapper_ng.py,reducer_ng.py,AFINN-111.txt,us_states.txt
-mapper mapper_ng.py -reducer 'reducer_ng.py --type states'
-input hdfs://sandbox.hortonworks.com/user/root/input/streaming/dump2.txt
-output hdfs://sandbox.hortonworks.com/user/root/output/streaming_02

11. Examine the results. The output of the execution will probably finish with a line such as this:
16/11/20 03:40:04 INFO streaming.StreamJob: Output directory: hdfs://sandbox.hortonworks.com/user/root/output/streaming_01

Thus, we get the directory:

[root@sandbox ~]# hdfs dfs -getmerge hdfs://sandbox.hortonworks.com/user/root/output/streaming_01 ./results_01.txt

12. If we want to chain two reduce operations, the output of the first reduce operation has to be used as input (it is
named part-00000). The mapper should be the cat command ('/bin/cat'). Thus, for option 10.b) above:

hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar -files mapper_ng.py,reducer_chained_01.py,AFINN-111.txt,us_states.txt -mapper mapper_ng.py -reducer 'reducer_chained_01.py --type states' -input hdfs://sandbox.hortonworks.com/user/root/input/streaming/dump.txt -output hdfs://sandbox.hortonworks.com/user/root/output/streaming_01

hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar -files reducer_chained_02.py -mapper '/bin/cat' -reducer reducer_chained_02.py -input hdfs://sandbox.hortonworks.com/user/root/output/streaming_01/part-00000 -output hdfs://sandbox.hortonworks.com/user/root/output/streaming_011


https://forums.aws.amazon.com/thread.jspa?threadID=34905
https://forums.aws.amazon.com/thread.jspa?messageID=588429

2016-11-24 18:34:10,136 WARN org.apache.hadoop.streaming.StreamJob (main): -jobconf option is deprecated, please use -D instead.